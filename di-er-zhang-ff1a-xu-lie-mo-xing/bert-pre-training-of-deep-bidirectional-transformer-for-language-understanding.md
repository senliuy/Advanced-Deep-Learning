# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## å‰è¨€

BERTï¼ˆ**B**idirectional **E**ncoder **R**epresentations from **T**ransformersï¼‰è¿‘æœŸæå‡ºä¹‹åï¼Œä½œä¸ºä¸€ä¸ªWord2Vecçš„æ›¿ä»£è€…ï¼Œå…¶åœ¨NLPé¢†åŸŸçš„11ä¸ªæ–¹å‘å¤§å¹…åˆ·æ–°äº†ç²¾åº¦ï¼Œå¯ä»¥è¯´æ˜¯è¿‘å¹´æ¥è‡ªæ®‹å·®ç½‘ç»œæœ€ä¼˜çªç ´æ€§çš„ä¸€é¡¹æŠ€æœ¯äº†ã€‚è®ºæ–‡çš„ä¸»è¦ç‰¹ç‚¹ä»¥ä¸‹å‡ ç‚¹ï¼š

1. ä½¿ç”¨äº†Transformer \[2\]ä½œä¸ºç®—æ³•çš„ä¸»è¦æ¡†æ¶ï¼ŒTrabsformerèƒ½æ›´å½»åº•çš„æ•æ‰è¯­å¥ä¸­çš„åŒå‘å…³ç³»ï¼›
2. ä½¿ç”¨äº†Mask Language Model\(MLM\) \[3\] å’Œ Next Sentence Prediction\(NSP\) çš„å¤šä»»åŠ¡è®­ç»ƒç›®æ ‡ï¼›
3. ä½¿ç”¨æ›´å¼ºå¤§çš„æœºå™¨è®­ç»ƒæ›´å¤§è§„æ¨¡çš„æ•°æ®ï¼Œä½¿BERTçš„ç»“æœè¾¾åˆ°äº†å…¨æ–°çš„é«˜åº¦ï¼Œå¹¶ä¸”Googleå¼€æºäº†BERTæ¨¡å‹ï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨BERTä½œä¸ºWord2Vecçš„è½¬æ¢çŸ©é˜µå¹¶é«˜æ•ˆçš„å°†å…¶åº”ç”¨åˆ°è‡ªå·±çš„ä»»åŠ¡ä¸­ã€‚

BERTçš„æœ¬è´¨ä¸Šæ˜¯é€šè¿‡åœ¨æµ·é‡çš„è¯­æ–™çš„åŸºç¡€ä¸Šè¿è¡Œè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä¸ºå•è¯å­¦ä¹ ä¸€ä¸ªå¥½çš„ç‰¹å¾è¡¨ç¤ºï¼Œæ‰€è°“è‡ªç›‘ç£å­¦ä¹ æ˜¯æŒ‡åœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æ•°æ®ä¸Šè¿è¡Œçš„ç›‘ç£å­¦ä¹ ã€‚åœ¨ä»¥åç‰¹å®šçš„NLPä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨BERTçš„ç‰¹å¾è¡¨ç¤ºä½œä¸ºè¯¥ä»»åŠ¡çš„è¯åµŒå…¥ç‰¹å¾ã€‚æ‰€ä»¥BERTæä¾›çš„æ˜¯ä¸€ä¸ªä¾›å…¶å®ƒä»»åŠ¡è¿ç§»å­¦ä¹ çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ ¹æ®ä»»åŠ¡å¾®è°ƒæˆ–è€…å›ºå®šä¹‹åä½œä¸ºç‰¹å¾æå–å™¨ã€‚BERTçš„æºç å’Œæ¨¡å‹10æœˆ31å·å·²ç»åœ¨Githubä¸Š[å¼€æº](https://github.com/google-research/bert)ï¼Œç®€ä½“ä¸­æ–‡å’Œå¤šè¯­è¨€æ¨¡å‹ä¹Ÿäº11æœˆ3å·å¼€æºã€‚

## 1. BERT è¯¦è§£

### 1.1 ç½‘ç»œæ¶æ„

BERTçš„ç½‘ç»œæ¶æ„ä½¿ç”¨çš„æ˜¯ã€ŠAttention is all you needã€‹ä¸­æå‡ºçš„å¤šå±‚Transformerç»“æ„ï¼Œå…¶æœ€å¤§çš„ç‰¹ç‚¹æ˜¯æŠ›å¼ƒäº†ä¼ ç»Ÿçš„RNNå’ŒCNNï¼Œé€šè¿‡Attentionæœºåˆ¶å°†ä»»æ„ä½ç½®çš„ä¸¤ä¸ªå•è¯çš„è·ç¦»è½¬æ¢æˆ1ï¼Œæœ‰æ•ˆçš„è§£å†³äº†NLPä¸­æ£˜æ‰‹çš„é•¿æœŸä¾èµ–é—®é¢˜ã€‚Transformerçš„ç»“æ„åœ¨NLPé¢†åŸŸä¸­å·²ç»å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œå¹¶ä¸”ä½œè€…å·²ç»å‘å¸ƒåœ¨TensorFlowçš„[tensor2tensor](https://github.com/tensorflow/tensor2tensor)åº“ä¸­ã€‚

Transformerçš„ç½‘ç»œæ¶æ„å¦‚å›¾1æ‰€ç¤ºï¼ŒTransformeræ˜¯ä¸€ä¸ªencoder-decoderçš„ç»“æ„ï¼Œç”±è‹¥å¹²ä¸ªç¼–ç å™¨å’Œè§£ç å™¨å †å å½¢æˆã€‚å›¾1çš„å·¦ä¾§éƒ¨åˆ†ä¸ºç¼–ç å™¨ï¼Œç”±Multi-Head Attentionå’Œä¸€ä¸ªå…¨è¿æ¥ç»„æˆï¼Œç”¨äºå°†è¾“å…¥è¯­æ–™è½¬åŒ–æˆç‰¹å¾å‘é‡ã€‚å³ä¾§éƒ¨åˆ†æ˜¯è§£ç å™¨ï¼Œå…¶è¾“å…¥ä¸ºç¼–ç å™¨çš„è¾“å‡ºä»¥åŠå·²ç»é¢„æµ‹çš„ç»“æœï¼Œç”±Masked Multi-Head Attention, Multi-Head Attentionä»¥åŠä¸€ä¸ªå…¨è¿æ¥ç»„æˆï¼Œç”¨äºè¾“å‡ºæœ€åç»“æœçš„æ¡ä»¶æ¦‚ç‡ã€‚å…³äºTransformerçš„è¯¦ç»†è§£æå‚è€ƒæˆ‘ä¹‹å‰æ€»ç»“çš„[æ–‡æ¡£](https://senliuy.gitbooks.io/advanced-deep-learning/content/di-er-zhang-ff1a-xu-lie-mo-xing/attention-is-all-you-need.html)ã€‚

<figure>
<img src="/assets/BERT_1.png" alt="å›¾1ï¼šBERT ä¸­é‡‡ç”¨çš„Transformerç½‘ç»œ" />
<figcaption>å›¾1ï¼šBERT ä¸­é‡‡ç”¨çš„Transformerç½‘ç»œ</figcaption>
</figure>


å›¾1ä¸­çš„å·¦ä¾§éƒ¨åˆ†æ˜¯ä¸€ä¸ªTransformer Blockï¼Œå¯¹åº”åˆ°å›¾2ä¸­çš„ä¸€ä¸ªâ€œTrmâ€ã€‚

<figure>
<img src="/assets/BERT_2.png" alt="å›¾2ï¼šBERTçš„ç½‘ç»œç»“æ„" />
<figcaption>å›¾2ï¼šBERTçš„ç½‘ç»œç»“æ„</figcaption>
</figure>


BERTæä¾›äº†ç®€å•å’Œå¤æ‚ä¸¤ä¸ªæ¨¡å‹ï¼Œå¯¹åº”çš„è¶…å‚æ•°åˆ†åˆ«å¦‚ä¸‹ï¼š

* $$\mathbf{BERT}_{\mathbf{BASE}}$$: L=12ï¼ŒH=768ï¼ŒA=12ï¼Œå‚æ•°æ€»é‡110Mï¼›

* $$\mathbf{BERT}_{\mathbf{LARGE}}$$: L=24ï¼ŒH=1024ï¼ŒA=16ï¼Œå‚æ•°æ€»é‡340Mï¼›

åœ¨ä¸Šé¢çš„è¶…å‚æ•°ä¸­ï¼ŒLè¡¨ç¤ºç½‘ç»œçš„å±‚æ•°ï¼ˆå³Transformer blocksçš„æ•°é‡ï¼‰ï¼ŒAè¡¨ç¤ºMulti-Head Attentionä¸­self-Attentionçš„æ•°é‡ï¼Œfilterçš„å°ºå¯¸æ˜¯4Hã€‚

è®ºæ–‡ä¸­è¿˜å¯¹æ¯”äº†BERTå’ŒGPT\[4\]å’ŒELMo\[5\]ï¼Œå®ƒä»¬ä¸¤ä¸ªçš„ç»“æ„å›¾å¦‚å›¾3æ‰€ç¤ºã€‚

<figure>
<img src="/assets/BERT_3.png" alt="å›¾3ï¼šOpenAI GPTå’ŒELMo" />
<figcaption>å›¾3ï¼šOpenAI GPTå’ŒELMo</figcaption>
</figure>

BERTå¯¹æ¯”è¿™ä¸¤ä¸ªç®—æ³•çš„ä¼˜ç‚¹æ˜¯åªæœ‰BERTè¡¨å¾ä¼š**åŸºäºæ‰€æœ‰å±‚ä¸­çš„å·¦å³ä¸¤ä¾§è¯­å¢ƒ**ã€‚BERTèƒ½åšåˆ°è¿™ä¸€ç‚¹å¾—ç›ŠäºTransformerä¸­Attentionæœºåˆ¶å°†ä»»æ„ä½ç½®çš„ä¸¤ä¸ªå•è¯çš„è·ç¦»è½¬æ¢æˆäº†1ã€‚

### 1.2 è¾“å…¥è¡¨ç¤º

BERTçš„è¾“å…¥çš„ç¼–ç å‘é‡ï¼ˆé•¿åº¦æ˜¯512ï¼‰æ˜¯3ä¸ªåµŒå…¥ç‰¹å¾çš„å•ä½å’Œï¼Œå¦‚å›¾4ï¼Œè¿™ä¸‰ä¸ªè¯åµŒå…¥ç‰¹å¾æ˜¯ï¼š

1. WordPiece åµŒå…¥\[6\]ï¼šWordPieceæ˜¯æŒ‡å°†å•è¯åˆ’åˆ†æˆä¸€ç»„æœ‰é™çš„å…¬å…±å­è¯å•å…ƒï¼Œèƒ½åœ¨å•è¯çš„æœ‰æ•ˆæ€§å’Œå­—ç¬¦çš„çµæ´»æ€§ä¹‹é—´å–å¾—ä¸€ä¸ªæŠ˜ä¸­çš„å¹³è¡¡ã€‚ä¾‹å¦‚å›¾4çš„ç¤ºä¾‹ä¸­â€˜playingâ€™è¢«æ‹†åˆ†æˆäº†â€˜playâ€™å’Œâ€˜ingâ€™ï¼›
2. ä½ç½®åµŒå…¥ï¼ˆPosition Embeddingï¼‰ï¼šä½ç½®åµŒå…¥æ˜¯æŒ‡å°†å•è¯çš„ä½ç½®ä¿¡æ¯ç¼–ç æˆç‰¹å¾å‘é‡ï¼Œä½ç½®åµŒå…¥æ˜¯å‘æ¨¡å‹ä¸­å¼•å…¥å•è¯ä½ç½®å…³ç³»çš„è‡³å…³é‡è¦çš„ä¸€ç¯ã€‚ä½ç½®åµŒå…¥çš„å…·ä½“å†…å®¹å‚è€ƒæˆ‘ä¹‹å‰çš„[åˆ†æ](https://senliuy.gitbooks.io/advanced-deep-learning/content/di-er-zhang-ff1a-xu-lie-mo-xing/attention-is-all-you-need.html)ï¼›
3. åˆ†å‰²åµŒå…¥ï¼ˆSegment Embeddingï¼‰ï¼šç”¨äºåŒºåˆ†ä¸¤ä¸ªå¥å­ï¼Œä¾‹å¦‚Bæ˜¯å¦æ˜¯Açš„ä¸‹æ–‡ï¼ˆå¯¹è¯åœºæ™¯ï¼Œé—®ç­”åœºæ™¯ç­‰ï¼‰ã€‚å¯¹äºå¥å­å¯¹ï¼Œç¬¬ä¸€ä¸ªå¥å­çš„ç‰¹å¾å€¼æ˜¯0ï¼Œç¬¬äºŒä¸ªå¥å­çš„ç‰¹å¾å€¼æ˜¯1ã€‚

æœ€åï¼Œè¯´æ˜ä¸€ä¸‹å›¾4ä¸­çš„ä¸¤ä¸ªç‰¹æ®Šç¬¦å·`[CLS]`å’Œ`[SEP]`ï¼Œå…¶ä¸­`[CLS]`è¡¨ç¤ºè¯¥ç‰¹å¾ç”¨äºåˆ†ç±»æ¨¡å‹ï¼Œå¯¹éåˆ†ç±»æ¨¡å‹ï¼Œè¯¥ç¬¦åˆå¯ä»¥çœå»ã€‚`[SEP]`è¡¨ç¤ºåˆ†å¥ç¬¦å·ï¼Œç”¨äºæ–­å¼€è¾“å…¥è¯­æ–™ä¸­çš„ä¸¤ä¸ªå¥å­ã€‚

<figure>
<img src="/assets/BERT_4.png" alt="å›¾4ï¼šBERTçš„è¾“å…¥ç‰¹å¾ã€‚ç‰¹å¾æ˜¯tokenåµŒå…¥ï¼Œä½ç½®åµŒå…¥å’Œåˆ†å‰²åµŒå…¥çš„å•ä½å’Œ" />
<figcaption>å›¾4ï¼šBERTçš„è¾“å…¥ç‰¹å¾ã€‚ç‰¹å¾æ˜¯tokenåµŒå…¥ï¼Œä½ç½®åµŒå…¥å’Œåˆ†å‰²åµŒå…¥çš„å•ä½å’Œ</figcaption>
</figure>

### 1.3 é¢„è®­ç»ƒä»»åŠ¡

BERTçš„ä»»åŠ¡æ˜¯ç”±ä¸¤ä¸ªè‡ªç›‘ç£ä»»åŠ¡ç»„æˆï¼Œå³MLMå’ŒNSPã€‚

#### 1.3.1 Task #1ï¼š Masked Language Model

Masked Language Modelï¼ˆMLMï¼‰å’Œæ ¸å¿ƒæ€æƒ³å–è‡ªWilson Tayloråœ¨1953å¹´å‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡\[7\]ã€‚æ‰€è°“MLMæ˜¯æŒ‡åœ¨è®­ç»ƒçš„æ—¶å€™éšå³ä»è¾“å…¥é¢„æ–™ä¸Šmaskæ‰ä¸€äº›å•è¯ï¼Œç„¶åé€šè¿‡çš„ä¸Šä¸‹æ–‡é¢„æµ‹è¯¥å•è¯ã€‚æ­£å¦‚ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ç®—æ³•å’ŒRNNåŒ¹é…é‚£æ ·ï¼ŒMLMçš„è¿™ä¸ªæ€§è´¨å’ŒTransformerçš„ç»“æ„æ˜¯éå¸¸åŒ¹é…çš„ã€‚

åœ¨BERTçš„å®éªŒä¸­ï¼Œ15%çš„WordPiece Tokenä¼šè¢«éšæœºMaskæ‰ã€‚åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œä¸€ä¸ªå¥å­ä¼šè¢«å¤šæ¬¡å–‚åˆ°æ¨¡å‹ä¸­ç”¨äºå‚æ•°å­¦ä¹ ï¼Œä½†æ˜¯Googleå¹¶æ²¡æœ‰åœ¨æ¯æ¬¡éƒ½maskæ‰è¿™äº›å•è¯ï¼Œè€Œæ˜¯åœ¨ç¡®å®šè¦Maskæ‰çš„å•è¯ä¹‹åï¼Œ80%çš„æ—¶å€™ä¼šç›´æ¥æ›¿æ¢ä¸º[Mask]ï¼Œ10%çš„æ—¶å€™å°†å…¶æ›¿æ¢ä¸ºå…¶å®ƒä»»æ„å•è¯ï¼Œ10%çš„æ—¶å€™ä¼šä¿ç•™åŸå§‹Tokenã€‚

* 80%ï¼š`my dog is hairy -> my dog is [mask]`
* 10%ï¼š`my dog is hairy -> my dog is apple`
* 10%ï¼š`my dog is hairy -> my dog is hairy`

è¿™ä¹ˆåšçš„åŸå› æ˜¯å¦‚æœå¥å­ä¸­çš„æŸä¸ªToken100%éƒ½ä¼šè¢«maskæ‰ï¼Œé‚£ä¹ˆåœ¨fine-tuningçš„æ—¶å€™æ¨¡å‹å°±ä¼šæœ‰ä¸€äº›æ²¡æœ‰è§è¿‡çš„å•è¯ã€‚åŠ å…¥éšæœºTokençš„åŸå› æ˜¯å› ä¸ºTransformerè¦ä¿æŒå¯¹æ¯ä¸ªè¾“å…¥tokençš„åˆ†å¸ƒå¼è¡¨å¾ï¼Œå¦åˆ™æ¨¡å‹å°±ä¼šè®°ä½è¿™ä¸ª[mask]æ˜¯token â€™hairyâ€˜ã€‚è‡³äºå•è¯å¸¦æ¥çš„è´Ÿé¢å½±å“ï¼Œå› ä¸ºä¸€ä¸ªå•è¯è¢«éšæœºæ›¿æ¢æ‰çš„æ¦‚ç‡åªæœ‰15%*10% =1.5%ï¼Œè¿™ä¸ªè´Ÿé¢å½±å“å…¶å®æ˜¯å¯ä»¥å¿½ç•¥ä¸è®¡çš„ã€‚

å¦å¤–æ–‡ç« æŒ‡å‡ºæ¯æ¬¡åªé¢„æµ‹15%çš„å•è¯ï¼Œå› æ­¤æ¨¡å‹æ”¶æ•›çš„æ¯”è¾ƒæ…¢ã€‚

#### 1.3.2 Task #2: Next Sentence Prediction

Next Sentence Predictionï¼ˆNSPï¼‰çš„ä»»åŠ¡æ˜¯åˆ¤æ–­å¥å­Bæ˜¯å¦æ˜¯å¥å­Açš„ä¸‹æ–‡ã€‚å¦‚æœæ˜¯çš„è¯ï¼Œè¾“å‡º

## Reference

\[1\] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\[J\]. arXiv preprint arXiv:1810.04805, 2018.

\[2\] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need \[C\]//Advances in Neural Information Processing Systems. 2017: 5998-6008.

\[3\] Wilson L Taylor. 1953. cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30\(4\):415â€“433.

\[4\] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI.

\[5\] Matthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL.

\[6\] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Googleâ€™s neural machine translation system: Bridging the gap between human and machine translation. arXiv:1609.08144.

\[7\] Wilson L Taylor. 1953. cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415â€“433.

